#!/bin/bash

# ============================================
# MedConnect Edge - Project Structure Fixer
# Fix missing files & improve organization
# ============================================

set -e

cd ~/MedConnect_Edge

echo "========================================"
echo "ðŸ”§ FIXING PROJECT STRUCTURE"
echo "========================================"
echo ""

# ============================================
# 1. GENERATE requirements.txt
# ============================================

echo "ðŸ“¦ Generating requirements.txt..."
./run.sh pip freeze > requirements.txt
echo "âœ… requirements.txt created ($(wc -l < requirements.txt) packages)"

# ============================================
# 2. CREATE .gitignore
# ============================================

echo ""
echo "ðŸš« Creating .gitignore..."

cat > .gitignore << 'EOF'
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual Environment
venv/
env/
ENV/
.venv

# IDE
.vscode/
.idea/
*.swp
*.swo
*~

# Jupyter Notebook
.ipynb_checkpoints
*.ipynb

# Model files & weights (large files)
*.h5
*.pb
*.onnx
*.tflite
*.pt
*.pth
*.bin
*.safetensors
models/checkpoints/*.pth
models/exports/*.onnx

# Dataset cache
.hf_cache/
datasets/raw/*/cache/
*.cache

# OS
.DS_Store
Thumbs.db

# Logs & Results
results/logs/*.log
results/metrics/*.csv
*.log

# Secrets & Config
.env
*.key
*.pem
kaggle.json

# Temporary files
*.tmp
*.temp
temp/
tmp/

# Large dataset files (track metadata only)
datasets/raw/**/*.zip
datasets/raw/**/*.tar.gz
datasets/processed/*.npy
datasets/augmented/*.npy
EOF

echo "âœ… .gitignore created"

# ============================================
# 3. CREATE .env.example
# ============================================

echo ""
echo "ðŸ”‘ Creating .env.example..."

cat > .env.example << 'EOF'
# HuggingFace API Token
HF_TOKEN=your_huggingface_token_here

# Kaggle API Credentials
KAGGLE_USERNAME=your_kaggle_username
KAGGLE_KEY=your_kaggle_key

# Model Configuration
MODEL_NAME=google/medgemma-1.5-4b-it
MAX_LENGTH=512
BATCH_SIZE=1

# Paths
DATASET_PATH=./datasets/raw/MedQA
MODEL_CACHE_DIR=./models/checkpoints
RESULTS_DIR=./results

# Inference Settings
USE_QUANTIZATION=true
DEVICE=cpu
EOF

echo "âœ… .env.example created"

# ============================================
# 4. EXPLORE MedQA Dataset Structure
# ============================================

echo ""
echo "ðŸ“Š Exploring MedQA dataset..."

# Check if actual data exists
if [ -d "datasets/raw/MedQA/IR" ]; then
    echo "âœ… MedQA IR folder found"
    
    # List subdirectories
    echo ""
    echo "MedQA structure:"
    find datasets/raw/MedQA/IR -maxdepth 2 -type d | head -10
    
    # Look for actual data files
    echo ""
    echo "Looking for medical QA files..."
    find datasets/raw/MedQA -name "*.jsonl" -o -name "*.json" | grep -v stopwords | head -10
    
else
    echo "âš ï¸ MedQA data structure incomplete"
fi

# ============================================
# 5. CREATE DATASET METADATA GENERATOR
# ============================================

echo ""
echo "ðŸ“ Creating metadata generator..."

cat > scripts/generate_metadata.py << 'EOF'
#!/usr/bin/env python3
"""
Generate metadata.json for tracking datasets
"""

import json
import os
from pathlib import Path
from datetime import datetime

def count_files(directory, extensions):
    """Count files with specific extensions"""
    count = 0
    for ext in extensions:
        count += len(list(Path(directory).rglob(f"*.{ext}")))
    return count

def get_dir_size(directory):
    """Get directory size in MB"""
    total = 0
    for path in Path(directory).rglob('*'):
        if path.is_file():
            total += path.stat().st_size
    return round(total / (1024 * 1024), 2)

def generate_metadata():
    """Generate metadata for all datasets"""
    
    base_dir = Path("datasets/raw")
    metadata = {
        "project": "MedConnect Edge",
        "generated_at": datetime.now().isoformat(),
        "datasets": {}
    }
    
    # Check MedQA
    medqa_dir = base_dir / "MedQA"
    if medqa_dir.exists():
        metadata["datasets"]["MedQA"] = {
            "path": str(medqa_dir),
            "type": "medical_qa",
            "format": "json/jsonl",
            "json_files": count_files(medqa_dir, ["json", "jsonl"]),
            "size_mb": get_dir_size(medqa_dir),
            "source": "https://github.com/jind11/MedQA",
            "status": "downloaded"
        }
    
    # Check for other datasets
    for dataset_dir in base_dir.iterdir():
        if dataset_dir.is_dir() and dataset_dir.name != "MedQA":
            metadata["datasets"][dataset_dir.name] = {
                "path": str(dataset_dir),
                "type": "unknown",
                "size_mb": get_dir_size(dataset_dir),
                "status": "downloaded"
            }
    
    # Save metadata
    output_path = Path("datasets/metadata.json")
    with open(output_path, 'w') as f:
        json.dump(metadata, f, indent=2)
    
    print(f"âœ… Metadata saved to: {output_path}")
    print(f"ðŸ“Š Total datasets: {len(metadata['datasets'])}")
    
    # Print summary
    print("\nðŸ“‹ Dataset Summary:")
    print("-" * 60)
    for name, info in metadata["datasets"].items():
        print(f"\n{name}:")
        print(f"  Type: {info.get('type', 'N/A')}")
        print(f"  Size: {info.get('size_mb', 0)} MB")
        if 'json_files' in info:
            print(f"  Files: {info['json_files']} JSON files")
        print(f"  Status: {info.get('status', 'unknown')}")

if __name__ == "__main__":
    generate_metadata()
EOF

chmod +x scripts/generate_metadata.py

# ============================================
# 6. CREATE scripts/ DIRECTORY
# ============================================

mkdir -p scripts

# ============================================
# 7. RUN METADATA GENERATOR
# ============================================

echo ""
echo "ðŸ”„ Generating dataset metadata..."
./run.sh python scripts/generate_metadata.py

# ============================================
# 8. CREATE IMPROVED README
# ============================================

echo ""
echo "ðŸ“ Updating README.md..."

cat > README.md << 'EOF'
# ðŸ¥ MedConnect Edge - AI Medical Assistant (Edge Devices)

**Lightweight AI-powered medical triage system optimized for edge devices**

> ðŸŽ¯ Kaggle Challenge Submission: HAI-DEF Foundation Models  
> ðŸ“± Target: CLI + Android deployment  
> ðŸ¤– Model: MedGemma (Google's medical LLM)

---

## ðŸ“‹ Project Overview

MedConnect Edge adalah sistem triase medis berbasis AI yang dirancang untuk:
- âœ… Analisis gejala & rekomendasi triase (URGENT/NON-URGENT)
- âœ… Penjelasan medis dengan bahasa natural (powered by MedGemma)
- âœ… Optimized untuk edge devices (low memory, CPU inference)
- âœ… Offline-first architecture

---

## ðŸš€ Quick Start

### 1. Activate Environment
```bash
cd ~/MedConnect_Edge
source venv/bin/activate
```

### 2. Run Baseline Triage CLI
```bash
./run.sh python src/inference/triage_cli.py \
  --symptoms "demam 4 hari, sakit kepala, nyeri otot, bintik merah"
```

**Output:**
```json
{
  "triage_level": "URGENT",
  "note": "Curiga infeksi (mis. dengue/DBD) butuh evaluasi.",
  "disclaimer": "Ini bukan diagnosis medis. Konsultasi dokter."
}
```

### 3. Run AI Explainer (MedGemma)
```bash
./run.sh python src/inference/medgemma_explain.py \
  --symptoms "demam tinggi 3 hari" \
  --triage-result "URGENT"
```

---

## ðŸ“ Project Structure

```
MedConnect_Edge/
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ raw/              # Raw datasets (MedQA, etc)
â”‚   â”œâ”€â”€ processed/        # Preprocessed data
â”‚   â””â”€â”€ metadata.json     # Dataset tracking
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ inference/        # Inference scripts
â”‚   â”‚   â”œâ”€â”€ triage_cli.py         # Baseline rule-based triage
â”‚   â”‚   â””â”€â”€ medgemma_explain.py   # AI explanation layer
â”‚   â”œâ”€â”€ preprocessing/    # Data preprocessing (TODO)
â”‚   â”œâ”€â”€ training/         # Model training (TODO)
â”‚   â””â”€â”€ utils/            # Utility functions
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ checkpoints/      # Training checkpoints
â”‚   â”œâ”€â”€ exports/          # Exported models (ONNX, TFLite)
â”‚   â””â”€â”€ tflite/           # Quantized TFLite models
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ logs/             # Training logs
â”‚   â”œâ”€â”€ metrics/          # Evaluation metrics
â”‚   â””â”€â”€ visualizations/   # Plots & charts
â”‚
â”œâ”€â”€ scripts/              # Utility scripts
â”‚   â””â”€â”€ generate_metadata.py
â”‚
â”œâ”€â”€ run.sh                # Convenience wrapper (no venv activation needed)
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ .env.example          # Environment variables template

```

---

## ðŸ› ï¸ Development Workflow

### Without Activating venv (Recommended)
```bash
# Install packages
./run.sh pip install package_name

# Run scripts
./run.sh python your_script.py

# Check Python version
./run.sh python --version
```

### With venv Activation (Traditional)
```bash
source venv/bin/activate
python your_script.py
deactivate
```

---

## ðŸ“Š Datasets

### MedQA Dataset
- **Path:** `datasets/raw/MedQA/`
- **Type:** Medical Question Answering
- **Source:** [jind11/MedQA](https://github.com/jind11/MedQA)
- **Status:** âœ… Downloaded
- **Usage:** Fine-tuning MedGemma for Indonesian medical context

View dataset info:
```bash
cat datasets/metadata.json
```

---

## ðŸ¤– Models

### Current Model: MedGemma
- **Model ID:** `google/medgemma-1.5-4b-it`
- **Size:** 4B parameters
- **Purpose:** Medical explanation & triage reasoning
- **Status:** âš ï¸ In progress (quantization needed for 8GB RAM)

---

## ðŸ”§ Installation & Setup

### System Requirements
- Python 3.12+
- 8GB RAM minimum
- 20GB disk space

### Initial Setup
```bash
cd ~/MedConnect_Edge
./run.sh pip install -r requirements.txt
```

### Environment Variables
```bash
cp .env.example .env
# Edit .env dengan token/credentials kamu
```

---

## ðŸ“ˆ Development Roadmap

- [x] Project structure setup
- [x] Baseline rule-based triage CLI
- [x] HuggingFace integration
- [x] MedQA dataset download
- [ ] MedGemma quantization (for 8GB RAM)
- [ ] Fine-tuning pipeline
- [ ] Model optimization (TFLite export)
- [ ] Android app development
- [ ] Kaggle submission

---

## ðŸ› Known Issues

### 1. MedGemma OOM on 8GB RAM
**Issue:** Model killed by OOM killer during load  
**Solution:** Use quantized model (GGUF) or cloud training

### 2. Dataset Parsing
**Issue:** MedQA format needs preprocessing  
**Solution:** Create preprocessing pipeline (in progress)

---

## ðŸ“ License

MIT License - See LICENSE file

---

## ðŸ‘¨â€ðŸ’» Author

**Alif Fauzan**  
- GitHub: [@AlifFauzan21](https://github.com/AlifFauzan21)
- Project: [medconnect-edge](https://github.com/AlifFauzan21/-medconnect-edge)

---

## ðŸ™ Acknowledgments

- Google HAI-DEF Team (MedGemma model)
- Kaggle Challenge organizers
- MedQA dataset creators

EOF

echo "âœ… README.md updated"

# ============================================
# 9. CREATE CHANGELOG
# ============================================

echo ""
echo "ðŸ“œ Creating CHANGELOG.md..."

cat > CHANGELOG.md << 'EOF'
# Changelog

All notable changes to MedConnect Edge project.

## [Unreleased]

### Added
- Project structure setup
- Baseline rule-based triage CLI
- MedGemma integration (in progress)
- HuggingFace authentication
- MedQA dataset download
- Convenience run.sh wrapper
- Comprehensive documentation

### In Progress
- MedGemma quantization for 8GB RAM
- Dataset preprocessing pipeline
- Training scripts

### Known Issues
- MedGemma OOM on 8GB RAM
- MedQA dataset needs parsing

## [0.1.0] - 2025-01-15

### Added
- Initial project setup
- Virtual environment configuration
- Basic inference scripts

EOF

echo "âœ… CHANGELOG.md created"

# ============================================
# 10. CREATE SETUP VERIFICATION SCRIPT
# ============================================

echo ""
echo "âœ… Creating verification script..."

cat > scripts/verify_setup.py << 'EOF'
#!/usr/bin/env python3
"""Verify project setup is correct"""

import sys
from pathlib import Path

def verify_structure():
    """Check if all required directories exist"""
    required_dirs = [
        "datasets/raw",
        "datasets/processed",
        "src/inference",
        "models/checkpoints",
        "results/logs",
        "scripts"
    ]
    
    print("ðŸ“ Verifying directory structure...")
    all_exist = True
    for dir_path in required_dirs:
        path = Path(dir_path)
        status = "âœ…" if path.exists() else "âŒ"
        print(f"  {status} {dir_path}")
        if not path.exists():
            all_exist = False
    
    return all_exist

def verify_files():
    """Check if required files exist"""
    required_files = [
        "run.sh",
        "requirements.txt",
        ".gitignore",
        ".env.example",
        "README.md",
        "src/inference/triage_cli.py"
    ]
    
    print("\nðŸ“„ Verifying required files...")
    all_exist = True
    for file_path in required_files:
        path = Path(file_path)
        status = "âœ…" if path.exists() else "âŒ"
        print(f"  {status} {file_path}")
        if not path.exists():
            all_exist = False
    
    return all_exist

def verify_packages():
    """Check if key packages are installed"""
    packages = [
        "transformers",
        "torch",
        "datasets",
        "huggingface_hub"
    ]
    
    print("\nðŸ“¦ Verifying Python packages...")
    all_installed = True
    for package in packages:
        try:
            __import__(package)
            print(f"  âœ… {package}")
        except ImportError:
            print(f"  âŒ {package}")
            all_installed = False
    
    return all_installed

def main():
    print("="*60)
    print("ðŸ” MedConnect Edge - Setup Verification")
    print("="*60 + "\n")
    
    checks = [
        verify_structure(),
        verify_files(),
        verify_packages()
    ]
    
    print("\n" + "="*60)
    if all(checks):
        print("âœ… ALL CHECKS PASSED!")
        print("ðŸš€ Your project is ready for development")
        return 0
    else:
        print("âš ï¸ SOME CHECKS FAILED")
        print("ðŸ’¡ Run ./fix_structure.sh to fix issues")
        return 1

if __name__ == "__main__":
    sys.exit(main())
EOF

chmod +x scripts/verify_setup.py

# ============================================
# 11. RUN VERIFICATION
# ============================================

echo ""
echo "ðŸ” Running setup verification..."
echo ""
./run.sh python scripts/verify_setup.py

# ============================================
# COMPLETION
# ============================================

echo ""
echo "========================================"
echo "âœ¨ PROJECT STRUCTURE FIXED!"
echo "========================================"
echo ""
echo "ðŸ“‹ Summary of changes:"
echo "  âœ… requirements.txt generated"
echo "  âœ… .gitignore created"
echo "  âœ… .env.example created"
echo "  âœ… README.md updated"
echo "  âœ… CHANGELOG.md created"
echo "  âœ… Dataset metadata generated"
echo "  âœ… Verification scripts added"
echo ""
echo "ðŸ“ New files created:"
echo "  - requirements.txt"
echo "  - .gitignore"
echo "  - .env.example"
echo "  - CHANGELOG.md"
echo "  - datasets/metadata.json"
echo "  - scripts/generate_metadata.py"
echo "  - scripts/verify_setup.py"
echo ""
echo "ðŸš€ Next steps:"
echo "  1. Review requirements.txt"
echo "  2. Check datasets/metadata.json"
echo "  3. Copy .env.example to .env and add your tokens"
echo "  4. Commit to git: git add . && git commit -m 'Project structure fixed'"
echo ""
echo "ðŸ’¡ To verify anytime: ./run.sh python scripts/verify_setup.py"
echo ""
